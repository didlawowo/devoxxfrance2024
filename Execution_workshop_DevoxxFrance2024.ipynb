{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Présentation du Dataset Titanic :\n",
        "\n",
        "**Contexte :** Le dataset Titanic est un ensemble de données classique en machine learning qui contient des informations sur les passagers du Titanic, y compris si un passager a survécu ou non.\n",
        "\n",
        "**Objectif :** Utiliser ce dataset pour prédire la survie des passagers à l'aide de techniques de machine learning et de deep learning.\n",
        "\n",
        "### Déroulement du Workshop :\n",
        "\n",
        "\n",
        "**Matériel Fourni :** Deux types de Jupyter Notebooks seront fournis :\n",
        "- **Notebook de Référence :** Contient tout le code nécessaire et les explications détaillées pour chaque étape.\n",
        "- **Notebook Interactif :** Comprend des bouts de code et des cellules vides pour permettre aux participants de coder pendant le workshop.\n",
        "\n",
        "**Activités Prévues :**\n",
        "- **Analyse des Données :** Explorer et visualiser les données pour comprendre les caractéristiques importantes.\n",
        "- **Nettoyage et Prétraitement :** Préparer les données pour le modèle en nettoyant et en appliquant des techniques de prétraitement.\n",
        "- **Construction et Entraînement du Modèle :** Utiliser Keras pour construire et entraîner un modèle de réseau de neurones.\n",
        "- **Optimisation des Paramètres :** Ajuster et affiner les paramètres du modèle pour améliorer la performance.\n",
        "- **Exploration d'Autres Modèles :** Discussion sur d'autres algorithmes de machine learning performants pour les données tabulaires.\n",
        "\n",
        "**Objectif de l'Atelier :** Fournir une expérience pratique enrichissante tout en démystifiant les concepts de deep learning et de machine learning à travers des exemples concrets.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kI1yfXJqtQhD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce bloc de code importe plusieurs bibliothèques essentielles utilisées pour l'analyse de données et la visualisation en Python :\n",
        "\n",
        "1. **`import seaborn as sns`** :\n",
        "   - `seaborn` est une bibliothèque de visualisation de données en Python qui est basée sur `matplotlib`. Elle permet de créer des graphiques de haute qualité de manière plus intuitive et avec moins de code. `seaborn` est particulièrement utile pour des graphiques statistiques et est conçu pour bien fonctionner avec des DataFrames `pandas`.\n",
        "\n",
        "2. **`import matplotlib.pyplot as plt`** :\n",
        "   - `matplotlib.pyplot` est une collection de fonctions dans la bibliothèque `matplotlib` qui rend `matplotlib` similaire à MATLAB. Chaque fonction de `pyplot` permet de modifier un aspect du graphique, par exemple, créer une figure, créer une zone de tracé dans la figure, tracer des lignes dans la zone de tracé, décorer le graphique avec des étiquettes, etc.\n",
        "\n",
        "3. **`import pandas as pd`** :\n",
        "   - `pandas` est une bibliothèque de manipulation et d'analyse de données, qui offre des structures de données et des opérations pour manipuler des tableaux numériques et des séries temporelles. C'est un outil indispensable en science des données pour le nettoyage et la préparation des données, avant l'analyse ou le modelage.\n",
        "\n",
        "4. **`import numpy as np`** :\n",
        "   - `numpy` est la bibliothèque fondamentale pour le calcul scientifique en Python. Elle fournit des objets de tableau multidimensionnel, des objets dérivés tels que les tableaux masqués et les matrices, et une assortiment de routines pour des opérations rapides sur des tableaux, y compris des opérations mathématiques, logiques, de manipulation de forme, de tri, de sélection, de transformations de Fourier, d'algèbre linéaire, de statistiques, et bien plus encore.\n",
        "\n",
        "En important ces bibliothèques, vous préparez un environnement de travail capable de gérer de vastes ensembles de données, de réaliser des calculs complexes et de visualiser les données de manière effective pour l'analyse ou la présentation des résultats.\n"
      ],
      "metadata": {
        "id": "EqD8H7mYtVTS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yl4cQl9o09KZ"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce bloc de code utilise la commande `wget` pour télécharger des fichiers depuis un dépôt GitHub directement dans l'environnement de travail local.\n"
      ],
      "metadata": {
        "id": "phzaBR_9vWeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/x-hub/devoxxfrance2024/raw/main/train.csv\n",
        "!wget https://github.com/x-hub/devoxxfrance2024/raw/main/test.csv\n",
        "!wget https://github.com/x-hub/devoxxfrance2024/raw/main/submission.csv"
      ],
      "metadata": {
        "id": "G3YNkyOnusBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce fragment de code utilise la bibliothèque `pandas`, souvent abrégée en `pd`, pour charger des données à partir de fichiers CSV dans des structures de données appelées DataFrames. Voici les détails de chaque commande :\n",
        "\n",
        "1. **`train = pd.read_csv('train.csv')`**\n",
        "   - Cette ligne de code charge les données à partir du fichier nommé `train.csv` dans un DataFrame appelé `train`. Le fichier `train.csv` contient typiquement des données qui seront utilisées pour entraîner un modèle de machine learning. Ces données peuvent inclure diverses caractéristiques des instances, telles que les âges, les sexes, les classes de billets, etc., ainsi que la variable cible, comme les étiquettes de survie dans le contexte du Titanic.\n",
        "\n",
        "2. **`test = pd.read_csv('test.csv')`**\n",
        "   - Cette ligne charge les données à partir du fichier `test.csv` dans un DataFrame appelé `test`. Le fichier `test.csv` est utilisé pour évaluer les performances du modèle formé sur l'ensemble d'entraînement. Il contient des caractéristiques similaires à celles de `train.csv` mais ne comprend généralement pas la variable cible, car l'objectif est de prédire cette variable avec le modèle formé.\n",
        "\n",
        "### Utilisation de ces DataFrames :\n",
        "- Les DataFrames `train` et `test` serviront à manipuler et préparer les données pour le modélisation. Ils permettent une gestion facile des données grâce à diverses fonctionnalités offertes par `pandas`, comme la manipulation de colonnes, le filtrage de lignes, et l'agrégation de données.\n",
        "- Ces structures sont essentielles pour séparer le processus d'apprentissage du processus de test, assurant ainsi que les évaluations du modèle sont réalisées de manière équitable et non biaisée.\n",
        "\n",
        "Ces commandes sont des étapes fondamentales en science des données pour charger et débuter l'exploration et la préparation des données en vue de leur utilisation dans des analyses plus complexes ou dans la formation de modèles prédictifs.\n"
      ],
      "metadata": {
        "id": "Z6aGlINx5ura"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "QvjJp1Pd-1X3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploration des données**\n",
        "On commence par voir si toutes les colonnes contiennent des données."
      ],
      "metadata": {
        "id": "QhnhGkO_CHiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.info()\n",
        "# cette instruction va nous imprimer une partie de notre dataset\n",
        "train"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4zXC_nrQCZ8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# meme chose pour les données de test\n",
        "test.info()\n",
        "test.describe()"
      ],
      "metadata": {
        "id": "YcAIROD7CbJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "Nous aurions besoin d’avoir une bonne compréhension de nos données. Ainsi, nous explorerions les taux de survie pour chaque colonne.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HG1T0V1XDZya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Le taux de survie global.\n",
        "Ce fragment de code utilise une bibliothèque de visualisation de données appelée `seaborn`, souvent abrégée en `sns`. Le but de ce code est de créer un diagramme à barres qui montre le nombre de personnes ayant survécu et non survécu dans un ensemble de données nommé `train`.\n",
        "\n",
        "1. **`sns.countplot()`** : Cette fonction est utilisée pour produire un graphique à barres. Chaque barre représente une catégorie, et la hauteur de chaque barre indique le nombre d'observations dans cette catégorie.\n",
        "\n",
        "2. **`data=train`** : Ceci spécifie que les données utilisées pour le graphique proviennent d'un DataFrame appelé `train`.\n",
        "\n",
        "3. **`x='Survived'`** : Ceci indique que les catégories à compter et à afficher sur l'axe des x sont basées sur la colonne `Survived` du DataFrame. Dans le contexte des données du Titanic, par exemple, cette colonne pourrait contenir des valeurs indiquant si les passagers ont survécu (`1`) ou non (`0`) au naufrage.\n",
        "\n",
        "En résumé, ce code est utilisé pour visualiser rapidement la distribution des valeurs dans une colonne spécifique (`Survived`) sous forme de diagramme à barres, permettant d'observer combien de personnes ont survécu par rapport à celles qui ne l'ont pas fait dans le jeu de données `train`.\n"
      ],
      "metadata": {
        "id": "P00O9qGb4yDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(data=train,x='Survived')"
      ],
      "metadata": {
        "id": "dQGXusNHbISd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# La classe\n",
        "\n",
        "1. **Première ligne : `sns.countplot(data=train, x='Pclass')`**\n",
        "   - Cette commande génère un graphique à barres montrant le nombre de passagers pour chaque classe de voyage disponible dans le DataFrame `train`.\n",
        "   - **`data=train`** indique que les données utilisées pour le graphique proviennent du DataFrame `train`.\n",
        "   - **`x='Pclass'`** spécifie que les catégories à compter et à afficher sur l'axe des x sont basées sur la colonne `Pclass`. Cette colonne représente les classes de voyage, comme la première classe, la deuxième classe, et la troisième classe.\n",
        "\n",
        "2. **Deuxième ligne : `sns.countplot(data=train, x='Pclass', hue='Survived')`**\n",
        "   - Cette commande étend la première en ajoutant une dimension supplémentaire au graphique à barres grâce à l'argument `hue`.\n",
        "   - **`hue='Survived'`** permet de diviser chaque barre en segments colorés qui représentent les catégories de la colonne `Survived`. Typiquement, cela pourrait montrer la proportion de passagers qui ont survécu (`1`) et ceux qui n'ont pas survécu (`0`) pour chaque classe de voyage.\n",
        "   - Le graphique résultant offre une visualisation claire de la répartition des taux de survie par classe, ce qui peut fournir des insights sur la relation entre la classe de voyage et les chances de survie.\n",
        "\n",
        "### Utilité de ces graphiques :\n",
        "Ces graphiques à barres sont particulièrement utiles pour l'analyse exploratoire des données, permettant de détecter des patterns ou des tendances dans la répartition des passagers par classe, ainsi que l'impact de la classe sur la survie lors d'événements comme le naufrage du Titanic. Cela aide à comprendre rapidement comment les différentes variables sont distribuées et interconnectées dans les données.\n",
        "\n"
      ],
      "metadata": {
        "id": "tQvmMYgn4leB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(data=train,x='Pclass')"
      ],
      "metadata": {
        "id": "B-KDL7Al_df7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(data=train,x='Pclass',hue='Survived')"
      ],
      "metadata": {
        "id": "3RspmhX2bgU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici ce que chaque partie du code fait :\n",
        "\n",
        "- `sns.barplot`: Cela indique à Python que nous voulons créer un graphique à barres à l'aide de la bibliothèque seaborn.\n",
        "\n",
        "- `data=train`: Cela spécifie que les données à utiliser pour créer le graphique sont stockées dans un DataFrame appelé 'train'. Vous devez donc avoir un DataFrame nommé 'train' contenant les données que vous souhaitez visualiser.\n",
        "\n",
        "- `x='Sex'`: Cela définit l'axe des abscisses du graphique. Dans ce cas, il représente la variable 'Sex' (sexe), ce qui signifie que chaque barre du graphique représentera un sexe différent.\n",
        "\n",
        "- `y='Survived'`: Cela définit l'axe des ordonnées du graphique. Dans ce cas, il représente la variable 'Survived' (survécu), ce qui signifie que la hauteur de chaque barre du graphique indiquera le nombre de personnes ayant survécu.\n",
        "\n",
        "- `hue=\"Pclass\"`: Cela ajoute une troisième dimension au graphique en utilisant la couleur pour représenter une autre variable, dans ce cas, la classe des passagers ('Pclass'). Cela signifie que chaque barre sera divisée en segments colorés correspondant à chaque classe de passagers.\n",
        "\n",
        "En résumé, ce code crée un graphique à barres montrant la relation entre le sexe des passagers, leur survie et leur classe. Les barres représentent le nombre de personnes de chaque sexe ayant survécu ou non, et chaque barre est divisée en segments colorés représentant les différentes classes de passagers. Cela peut être utile pour visualiser les différences de survie entre les sexes et les classes.\n"
      ],
      "metadata": {
        "id": "OTjvMwOe6ZhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot( data=train,x='Sex',y='Survived', hue=\"Pclass\")"
      ],
      "metadata": {
        "id": "J0lSJwRAE-uC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#L'age\n",
        "\n",
        "- `sns.histplot`: Cela indique à Python que nous voulons créer un histogramme à l'aide de la bibliothèque seaborn.\n",
        "\n",
        "- `data=train`: Cela spécifie que les données à utiliser pour créer l'histogramme sont stockées dans un DataFrame appelé 'train'. Vous devez donc avoir un DataFrame nommé 'train' contenant les données que vous souhaitez visualiser.\n",
        "\n",
        "- `x='Age'`: Cela définit la variable que nous voulons représenter sur l'axe des abscisses de l'histogramme, dans ce cas, il s'agit de l'âge des passagers.\n",
        "\n",
        "En résumé, ce code crée un histogramme montrant la distribution des âges des passagers. L'axe des abscisses représente les différentes tranches d'âge, tandis que l'axe des ordonnées représente le nombre de passagers dans chaque tranche d'âge. Cela peut être utile pour analyser la répartition des âges dans l'ensemble de données.\n"
      ],
      "metadata": {
        "id": "dL5flPGD61jJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(data=train,x='Age')"
      ],
      "metadata": {
        "id": "t6XuJ4wEcpen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un histogramme avec une distinction de couleur basée sur la survie des passagers. Voici ce que chaque partie du code fait :\n",
        "\n",
        "- `sns.histplot`: Cela indique à Python que nous voulons créer un histogramme à l'aide de la bibliothèque seaborn.\n",
        "\n",
        "- `data=train`: Cela spécifie que les données à utiliser pour créer l'histogramme sont stockées dans un DataFrame appelé 'train'. Vous devez donc avoir un DataFrame nommé 'train' contenant les données que vous souhaitez visualiser.\n",
        "\n",
        "- `x='Age'`: Cela définit la variable que nous voulons représenter sur l'axe des abscisses de l'histogramme, dans ce cas, il s'agit de l'âge des passagers.\n",
        "\n",
        "- `hue='Survived'`: Cela ajoute une distinction de couleur basée sur la variable 'Survived' (survécu), ce qui signifie que l'histogramme sera coloré différemment pour les passagers ayant survécu et ceux n'ayant pas survécu.\n",
        "\n",
        "En résumé, ce code crée un histogramme montrant la distribution des âges des passagers, avec une distinction de couleur pour les passagers ayant survécu et ceux n'ayant pas survécu. Cela permet de visualiser si la distribution des âges varie en fonction de la survie des passagers.\n"
      ],
      "metadata": {
        "id": "6MfJ1Ghx7I6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(data=train,x='Age', hue='Survived')"
      ],
      "metadata": {
        "id": "z8VlUTmk-Idw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Porte d'embarquement\n"
      ],
      "metadata": {
        "id": "234_iIEn6lRW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sP6u0QyHceHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Le sexe"
      ],
      "metadata": {
        "id": "cOrDjLBv7XoK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jlrtkYEpIZrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# La famille\n",
        "\n"
      ],
      "metadata": {
        "id": "3JIdjyiF7lm7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K4cHHbXXeD9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Préparation des données"
      ],
      "metadata": {
        "id": "HknflKzFPw2T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce script Python prépare un ensemble de données pour l'analyse en fusionnant deux ensembles de données séparés, `train` et `test`, et en effectuant plusieurs étapes de nettoyage et de transformation des données :\n",
        "\n",
        "1. **Marquage des données** : Les ensembles de données `train` et `test` sont d'abord marqués avec une nouvelle colonne `is_train` pour indiquer si une ligne appartient à l'ensemble d'entraînement (1) ou à l'ensemble de test (0). Cela permet de les différencier facilement après les avoir fusionnés en un seul DataFrame.\n",
        "\n",
        "2. **Fusion des données** : Les ensembles `train` et `test` sont fusionnés en un seul DataFrame `all_data` pour permettre un traitement uniforme lors de la préparation des données.\n",
        "\n",
        "3. **Imputation des valeurs manquantes** :\n",
        "   - Les valeurs manquantes dans la colonne `Age` sont remplacées par la médiane de `Age` pour éviter des biais introduits par des valeurs aberrantes.\n",
        "   - Les valeurs manquantes dans la colonne `Fare` sont également remplacées par la médiane de `Fare`.\n",
        "   - Les valeurs manquantes dans la colonne `Embarked` sont remplacées par 'U', assumant que l'absence de données peut être classée sous une nouvelle catégorie inconnue 'U'.\n",
        "\n",
        "4. **Création de nouvelles caractéristiques** :\n",
        "   - `Age_class` : Les âges sont classés en catégories [Enfant, Ado, Adulte, Aînés] en utilisant les intervalles d'âge spécifiés.\n",
        "   - `Family_size` : Une nouvelle caractéristique qui est la somme de `SibSp` (nombre de frères et sœurs/conjoint à bord) et `Parch` (nombre de parents/enfants à bord).\n",
        "\n",
        "5. **Transformation des caractéristiques catégorielles en indicateurs** :\n",
        "   - Pour chaque colonne catégorielle dans `cols` (Sexe, Classe, Embarked, Age_class), les variables indicatrices sont créées. Ces indicateurs sont des colonnes binaires (0 ou 1) qui correspondent à la présence de chaque valeur possible dans la colonne originale.\n",
        "\n",
        "6. **Nettoyage final des données** :\n",
        "   - Les colonnes qui ne sont plus nécessaires, telles que `Cabin`, `Ticket`, `Name`, `Age` et les colonnes originelles dans `cols`, sont supprimées pour simplifier le modèle.\n",
        "\n",
        "7. **Résumé des données** :\n",
        "   - `all_data.info()` est utilisé pour afficher un résumé des données, y compris les types de données et le nombre de valeurs non nulles, ce qui est utile pour vérifier que les étapes de prétraitement ont été correctement appliquées.\n",
        "\n",
        "Ce prétraitement est essentiel pour préparer les données pour les modèles de machine learning, en s'assurant que les données sont complètes, formatées correctement, et enrichies avec des caractéristiques potentiellement utiles\n"
      ],
      "metadata": {
        "id": "PyoEiNbKP3Kx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajout d'une colonne 'is_train' avec la valeur 1 pour les données d'entraînement\n",
        "train['is_train'] = 1\n",
        "# Ajout d'une colonne 'is_train' avec la valeur 0 pour les données de test\n",
        "test['is_train'] = 0\n",
        "# Concaténation des données d'entraînement et de test\n",
        "all_data = pd.concat([train, test], axis=0)\n",
        "\n",
        "# Remplissage des valeurs manquantes pour l'âge avec la médiane\n",
        "all_data.Age = all_data.Age.fillna(all_data.Age.median())\n",
        "\n",
        "# Remplissage des valeurs manquantes pour le tarif avec la médiane\n",
        "all_data.Fare = all_data.Fare.fillna(all_data.Fare.median())\n",
        "\n",
        "# Remplissage des valeurs manquantes pour l'embarquement avec 'U' (inconnu)\n",
        "all_data.Embarked = all_data.Embarked.fillna('U')\n",
        "\n",
        "# Création de la variable 'Age_class' en découpant l'âge en catégories\n",
        "all_data['Age_class'] = pd.cut(all_data.Age, bins=[0, 15, 40, 60, 80], labels=['Enfant', 'Ado', 'Adulte', 'Aînées'])\n",
        "\n",
        "# Calcul de la taille de la famille en ajoutant les colonnes 'SibSp' et 'Parch'\n",
        "all_data['Family_size'] = all_data.SibSp + all_data.Parch\n",
        "\n",
        "# Ce code extrait le titre de chaque personne à partir de la colonne 'Name'\n",
        "all_data['Title'] = all_data['Name'].apply(lambda name: name.split(',')[1].split('.')[0].strip())\n",
        "\n",
        "# Colonnes à traiter pour créer des variables factices\n",
        "cols = ['Sex', 'Pclass', 'Embarked', 'Age_class','Title']\n",
        "\n",
        "# Création de variables factices pour chaque colonne spécifiée dans 'cols'\n",
        "all_data = pd.get_dummies(all_data, columns=cols,dtype=int)\n",
        "\n",
        "\n",
        "# Suppression des colonnes non nécessaires\n",
        "all_data = all_data.drop(columns=['Cabin', 'Ticket', 'Name', 'Age'] + cols)\n",
        "\n",
        "# Affichage des informations sur le DataFrame all_data\n",
        "all_data.info()\n"
      ],
      "metadata": {
        "id": "rOYEBF5QY68u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Heatmap\n",
        "La heatmap des corrélations est un outil visuel puissant en analyse de données, particulièrement utile pour détecter des relations potentielles ou des redondances entre les caractéristiques. Les coefficients de corrélation peuvent aider à identifier les caractéristiques qui ont une forte influence l'une sur l'autre, ce qui peut être crucial pour la sélection des caractéristiques lors de la modélisation prédictive.\n",
        "\n",
        "\n",
        "L'interprétation de la carte de chaleur (heatmap) dans le contexte du dataset du Titanic permet de comprendre les relations de corrélation entre différentes variables. Voici quelques points clés pour interpréter les résultats :\n",
        "\n",
        "1. **Couleur et intensité** :\n",
        "   - Les couleurs sur la carte de chaleur représentent le degré de corrélation entre les variables. Une couleur plus foncée ou une intensité plus élevée indique une corrélation plus forte, tandis qu'une couleur plus claire ou une intensité plus faible indique une corrélation plus faible.\n",
        "\n",
        "2. **Interprétation des valeurs de corrélation** :\n",
        "   - Une corrélation positive indique que les variables évoluent dans la même direction : si l'une augmente, l'autre a tendance à augmenter également, et vice versa.\n",
        "   - Une corrélation négative indique que les variables évoluent dans des directions opposées : si l'une augmente, l'autre a tendance à diminuer, et vice versa.\n",
        "\n",
        "3. **Exemple avec les variables 'SibSp' et 'Parch'** :\n",
        "   - Si la case correspondant à ces deux variables a une couleur foncée, cela signifie qu'elles sont fortement corrélées entre elles. Cela pourrait indiquer qu'il y a une relation entre le nombre de frères et sœurs/époux(ses) à bord ('SibSp') et le nombre de parents/enfants à bord ('Parch').\n",
        "\n",
        "4. **Décision de conservation ou de suppression de variables** :\n",
        "   - À partir des informations fournies par la heatmap, vous pouvez décider de conserver ou de laisser tomber certaines variables dans votre analyse. Par exemple, si deux variables sont fortement corrélées, vous pouvez choisir de ne conserver qu'une seule pour éviter la redondance dans votre modèle.\n",
        "\n",
        "En résumé, la heatmap est un outil utile pour visualiser les relations entre les variables dans un dataset, ce qui peut aider à prendre des décisions sur les variables à inclure dans une analyse ou un modèle.\n"
      ],
      "metadata": {
        "id": "zajDHP6OQMDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(18,9))\n",
        "sns.heatmap(all_data[all_data.is_train==1].drop(columns=['is_train']).corr(),annot=True)"
      ],
      "metadata": {
        "id": "f5gog8mOTsfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# La validation adversariale\n",
        "\n",
        "La validation adversariale est une technique utilisée en apprentissage automatique pour évaluer la similitude entre les distributions des données d'entraînement et de test. L'objectif est de détecter et de quantifier les différences entre ces deux ensembles de données, ce qui peut être crucial pour la généralisation efficace d'un modèle.\n",
        "\n",
        "Dans le code fourni, la validation adversariale est mise en œuvre de la manière suivante :\n",
        "\n",
        "1. **Chargement des ensembles de données** :\n",
        "   - Les ensembles de données d'entraînement et de test sont chargés à partir de fichiers CSV ('train.csv' et 'test.csv').\n",
        "\n",
        "2. **Combinaison des ensembles de données** :\n",
        "   - Les ensembles de données d'entraînement et de test sont combinés en un seul ensemble de données 'combined_data'.\n",
        "\n",
        "3. **Séparation des variables caractéristiques et de la variable cible** :\n",
        "   - Les variables caractéristiques sont extraites de 'combined_data' en supprimant les colonnes 'PassengerId', 'Survived' et 'is_train'. La variable cible 'is_train' indique si une observation appartient à l'ensemble de données d'entraînement ou de test.\n",
        "\n",
        "4. **Division en ensembles d'entraînement et de validation** :\n",
        "   - Les données sont divisées en ensembles d'entraînement et de validation à l'aide de la fonction `train_test_split()` de scikit-learn.\n",
        "\n",
        "5. **Entraînement du classifieur** :\n",
        "   - Un classifieur RandomForest est entraîné sur les données d'entraînement pour prédire si une observation appartient à l'ensemble de données d'entraînement ou de test.\n",
        "\n",
        "6. **Prédiction des probabilités pour l'ensemble de validation** :\n",
        "   - Les probabilités prédites pour l'ensemble de validation sont calculées à l'aide de la méthode `predict_proba()` du classifieur RandomForest.\n",
        "\n",
        "7. **Calcul du score ROC AUC** :\n",
        "   - Le score ROC AUC est calculé en comparant les probabilités prédites avec les vérités terrain de l'ensemble de validation.\n",
        "\n",
        "La validation adversariale permet de détecter les biais potentiels dans un modèle et de s'assurer qu'il généralise bien sur des données non vues auparavant. En évaluant la similitude entre les distributions des données d'entraînement et de test, cette technique peut aider à identifier les problèmes de surapprentissage et à améliorer la robustesse et la généralisation des modèles.\n"
      ],
      "metadata": {
        "id": "Q7_OBcX-RLzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Charger les ensembles de données d'entraînement et de test\n",
        "train_data = pd.read_csv('train.csv')\n",
        "test_data = pd.read_csv('test.csv')\n",
        "\n",
        "\n",
        "# Combinaison des ensembles de données\n",
        "combined_data = all_data\n",
        "\n",
        "# Séparer les variables caractéristiques et la variable cible\n",
        "X = combined_data.drop(['PassengerId','Survived', 'is_train'], axis=1)\n",
        "y = combined_data['is_train']\n",
        "\n",
        "\n",
        "# Diviser les données en ensembles d'entraînement et de validation pour le classifieur\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Entraîner un classifieur pour prédire si une ligne appartient à l'ensemble d'entraînement ou de test\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Prédire les probabilités pour l'ensemble de validation\n",
        "valid_pred_probs = clf.predict_proba(X_valid)[:, 1]\n",
        "\n",
        "# Calculer le score ROC AUC\n",
        "auc_score = roc_auc_score(y_valid, valid_pred_probs)\n",
        "print(\"ROC AUC Score:\", auc_score)\n"
      ],
      "metadata": {
        "id": "23JRkQP2RVQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Un premier model\n",
        "Ce script illustre la création, la compilation et l'entraînement d'un modèle de réseau de neurones utilisant TensorFlow et Keras pour une tâche de classification binaire. Voici les détails de chaque étape du code :\n",
        "\n",
        "1. **Importation des bibliothèques nécessaires** :\n",
        "   - `from tensorflow.keras import Sequential` : Importe la classe `Sequential` de Keras, qui permet de construire des modèles couche par couche.\n",
        "   - `from tensorflow.keras.layers import Dense, Dropout` : Importe les couches `Dense` pour les connexions neuronales denses et `Dropout` pour la régularisation.\n",
        "   - `from sklearn.model_selection import train_test_split` : Importe la fonction pour diviser les données en ensembles d'entraînement et de test.\n",
        "\n",
        "2. **Préparation des données** :\n",
        "   - Sélection et préparation des variables explicatives `X` et de la variable cible `Y` à partir de l'ensemble de données. Les colonnes non pertinentes ou spécifiques sont exclues pour se concentrer sur les caractéristiques significatives.\n",
        "\n",
        "3. **Division des données** :\n",
        "   - `train_test_split` est utilisé pour diviser les données en ensembles d'entraînement et de test, avec 20% des données réservées pour le test. Cela aide à évaluer la performance du modèle sur des données non vues.\n",
        "\n",
        "4. **Construction du modèle** :\n",
        "   - Un modèle `Sequential` est défini avec deux couches `Dense` pour l'apprentissage des caractéristiques, chacune suivie par une activation ReLU.\n",
        "   - Une couche `Dropout` est ajoutée entre les couches `Dense` pour réduire le risque de surapprentissage en \"ignorant\" aléatoirement certains neurones pendant l'entraînement.\n",
        "\n",
        "5. **Compilation du modèle** :\n",
        "   - Le modèle est compilé avec la fonction de perte `binary_crossentropy`, adaptée aux problèmes de classification binaire. L'optimiseur `rmsprop` est utilisé pour l'ajustement des poids, et la métrique `accuracy` pour évaluer la performance.\n",
        "\n",
        "6. **Entraînement du modèle** :\n",
        "   - `model.fit` est utilisé pour entraîner le modèle sur les données d'entraînement, avec un nombre spécifié d'époques (50 dans ce cas). Les données de test sont utilisées comme données de validation pour observer la performance du modèle sur un ensemble de données distinct pendant l'entraînement.\n",
        "\n",
        "7. **Évaluation du modèle** :\n",
        "   - La fonction `evaluate_model`, non définie dans ce script mais appelée à la fin, serait utilisée pour évaluer la performance finale du modèle sur l'ensemble des données, probablement incluant un calcul de précision sur un ensemble de données de test ou de validation externe.\n",
        "\n",
        "Ce code constitue un exemple typique de l'utilisation des réseaux de neurones pour la classification dans des applications pratiques, illustrant les étapes de base de la construction, de l'entraînement et de l'évaluation des modèles de machine learning.\n"
      ],
      "metadata": {
        "id": "CHQ6zTKqTegm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sélection des features et de la target à partir du DataFrame all_data\n",
        "# pour les données d'entraînement (is_train == 1)\n",
        "X = all_data[all_data.is_train == 1].drop(columns=['is_train', 'Survived', 'PassengerId', 'SibSp', 'Parch'])\n",
        "Y = all_data[all_data.is_train == 1].Survived\n",
        "\n",
        "# Division des données en ensembles d'entraînement et de test\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=29)\n",
        "\n",
        "# Définition du modèle séquentiel\n",
        "model = Sequential([\n",
        "    Dense(32, activation='relu'),     # Couche dense avec 32 neurones et fonction d'activation relu\n",
        "    Dense(32, activation='relu'),     # Couche dense avec 64 neurones et fonction d'activation relu\n",
        "    Dense(1, activation='sigmoid')    # Couche dense avec 1 neurone et fonction d'activation sigmoid pour la classification binaire\n",
        "])\n",
        "\n",
        "# Compilation du modèle\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "4nCOYttFB404"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce code trace un graphique pour visualiser l'évolution de l'accuracy (précision) et de la validation accuracy d'un modèle au fil des epochs (itérations de l'entraînement). Voici comment interpréter le graphique :\n",
        "\n",
        "- **Axes** :\n",
        "  - L'axe des abscisses représente le nombre d'epochs, c'est-à-dire le nombre d'itérations de l'entraînement du modèle.\n",
        "  - L'axe des ordonnées représente les valeurs de l'accuracy et de la validation accuracy.\n",
        "\n",
        "- **Points bleus (accuracy)** :\n",
        "  - Chaque point bleu sur la courbe indique l'accuracy du modèle sur les données d'entraînement à cet epoch particulier. Plus les points bleus sont élevés, meilleure est l'accuracy du modèle sur les données d'entraînement.\n",
        "\n",
        "- **Ligne bleue (validation accuracy)** :\n",
        "  - La ligne bleue relie les valeurs de la validation accuracy du modèle à chaque epoch. Elle montre comment la performance du modèle sur les données de validation évolue au fil de l'entraînement. Une augmentation de la validation accuracy indique que le modèle généralise mieux et ne surapprend pas trop aux données d'entraînement.\n",
        "\n",
        "- **Légende** :\n",
        "  - La légende affiche les étiquettes des courbes. 'accuracy' correspond à l'accuracy sur les données d'entraînement et 'val accuracy' correspond à la validation accuracy.\n",
        "\n",
        "Interpréter le graphique :\n",
        "- Idéalement, on souhaite que les courbes de l'accuracy et de la validation accuracy convergent vers un plateau. Cela signifie que le modèle apprend efficacement sur les données d'entraînement et généralise bien sur des données qu'il n'a pas vues pendant l'entraînement.\n",
        "- Si la courbe de l'accuracy continue d'augmenter tandis que la courbe de la validation accuracy commence à diminuer, cela peut indiquer un surapprentissage, où le modèle apprend trop les données d'entraînement spécifiques et ne généralise pas bien sur de nouvelles données.\n",
        "\n",
        "En résumé, ce graphique est un outil précieux pour évaluer la performance et le comportement du modèle pendant l'entraînement, ainsi que pour détecter d'éventuels problèmes de surapprentissage.\n"
      ],
      "metadata": {
        "id": "C_A9M7nuOdXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Récupération de l'historique de l'entraînement pour l'accuracy et la validation accuracy\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "# Création d'une plage d'epochs pour l'axe des x\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# Tracé de la courbe de l'accuracy et de la validation accuracy\n",
        "plt.plot(epochs, acc, 'bo', label='accuracy')          # Points bleus pour l'accuracy\n",
        "plt.plot(epochs, val_acc, 'b', label='val accuracy')   # Ligne bleue pour la validation accuracy\n",
        "\n",
        "# Ajout de la légende et affichage du graphique\n",
        "plt.legend()  # Affichage de la légende\n",
        "plt.show()    # Affichage du graphique\n"
      ],
      "metadata": {
        "id": "1lEDvUoUEpmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# la validation croisée K-Fold\n",
        "\n",
        "Ce script démontre comment utiliser la validation croisée K-Fold avec un modèle de réseau de neurones en utilisant Keras et scikit-learn. Voici une explication détaillée de chaque étape du code :\n",
        "\n",
        "1. **Importation des bibliothèques nécessaires** :\n",
        "   - `from sklearn.model_selection import KFold` importe la classe `KFold` qui permet de réaliser la validation croisée.\n",
        "\n",
        "2. **Configuration de la validation croisée** :\n",
        "   - `n_splits = 5` définit le nombre de plis (folds) à utiliser, ici 5, ce qui est un choix courant pour la validation croisée.\n",
        "   - `kfold = KFold(n_splits=n_splits, shuffle=True, random_state=29)` crée un objet `KFold` pour gérer les divisions des données, avec un mélange activé pour une distribution aléatoire des données.\n",
        "\n",
        "3. **Initialisation de la liste pour stocker les précisions** :\n",
        "   - `accuracies = []` sert à stocker la précision de chaque pli.\n",
        "\n",
        "4. **Boucle sur les plis générés par KFold** :\n",
        "   - À chaque itération, un nouveau modèle est créé et compilé avec une architecture spécifique incluant des couches `Dense` et `Dropout`.\n",
        "   - Le modèle est entraîné sur le sous-ensemble d'entraînement et évalué sur le sous-ensemble de test pour chaque pli.\n",
        "\n",
        "5. **Création et compilation du modèle** :\n",
        "   - Le modèle comprend des couches alternées `Dense` pour l'apprentissage et `Dropout` pour la régularisation afin d'éviter le surapprentissage.\n",
        "\n",
        "6. **Entraînement et évaluation du modèle** :\n",
        "   - `model.fit` entraîne le modèle sur les données d'entraînement de chaque pli, et `model.evaluate` évalue le modèle sur les données de test.\n",
        "   - Les précisions obtenues sont ajoutées à la liste `accuracies`.\n",
        "\n",
        "7. **Affichage de la précision pour chaque pli** :\n",
        "   - La précision de chaque pli est affichée.\n",
        "\n",
        "8. **Calcul et affichage de la précision moyenne et de l'écart-type** :\n",
        "   - Après la fin de tous les plis, la précision moyenne et l'écart-type sont calculés et affichés pour donner une idée de la performance globale du modèle et de sa stabilité sur différentes parties des données.\n",
        "\n",
        "Ce processus aide à comprendre comment le modèle se comporte de manière générale sur le jeu de données, plutôt que dépendant d'une seule division des données en entraînement/test, fournissant ainsi une estimation plus robuste de sa capacité à généraliser sur de nouvelles données.\n"
      ],
      "metadata": {
        "id": "1ngniNweS6UU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Nombre de plis\n",
        "n_splits = 5\n",
        "\n",
        "# K-Fold Cross-Validator\n",
        "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=29)\n",
        "\n",
        "# Liste pour stocker l'accuracy de chaque pli\n",
        "accuracies = []\n",
        "\n",
        "for train, test in kfold.split(X):\n",
        "    # Création d'un nouveau modèle (même architecture)\n",
        "    model = Sequential([\n",
        "        Dense(16, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Compilation du modèle\n",
        "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "    # Entraînement du modèle\n",
        "    model.fit(X.iloc[train], Y.iloc[train], epochs=50, verbose=0)\n",
        "\n",
        "    # Évaluation du modèle\n",
        "    scores = model.evaluate(X.iloc[test], Y.iloc[test], verbose=0)\n",
        "    print(f\"Accuracy: {scores[1]*100:.2f}%\")\n",
        "    accuracies.append(scores[1] * 100)\n",
        "\n",
        "# Moyenne et écart-type de l'accuracy\n",
        "mean_accuracy = np.mean(accuracies)\n",
        "std_deviation = np.std(accuracies)\n",
        "print(f\"Mean Accuracy: {mean_accuracy:.2f}%\")\n",
        "print(f\"Standard Deviation: {std_deviation:.2f}%\")\n"
      ],
      "metadata": {
        "id": "pYD8L0O1iud_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Autres méthodes / Algorithmes\n",
        "Il est important de noter que pour résoudre le dataset du Titanic, il n'est pas nécessaire de se limiter uniquement aux réseaux neuronaux profonds (DNN). Il existe plusieurs autres algorithmes de machine learning qui peuvent également être efficaces. Lors de cet atelier, nous allons explorer différentes approches pour résoudre ce problème, notamment :\n",
        "\n",
        "- **Régression Logistique (LogisticRegression)** : Un algorithme classique de classification qui modélise la probabilité d'appartenance à une classe à l'aide d'une fonction logistique.\n",
        "  \n",
        "- **XGBoost (XGBClassifier)** : Un algorithme d'ensemble basé sur des arbres de décision, connu pour sa haute performance et sa capacité à gérer des ensembles de données complexes.\n",
        "\n",
        "- **SVM (SVC)** : Les machines à vecteurs de support sont des algorithmes de classification efficaces, particulièrement adaptés aux données avec des frontières de décision complexes et non linéaires.\n",
        "\n",
        "- **Arbre de Décision (DecisionTreeClassifier)** : Un modèle simple et interprétable qui divise récursivement l'ensemble de données en sous-ensembles homogènes en fonction des valeurs de caractéristiques.\n",
        "\n",
        "Ces différentes approches offrent une diversité de techniques pour résoudre le problème du Titanic, chacune ayant ses propres avantages et inconvénients.\n"
      ],
      "metadata": {
        "id": "vhbDPCkYP8Kt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import tree\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "svc = SVC(probability = True)\n",
        "cv = cross_val_score(svc,X_train,Y_train,cv=5)\n",
        "print(cv)\n",
        "print(cv.mean())"
      ],
      "metadata": {
        "id": "P3KO26z9dMGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(random_state =1)\n",
        "cv = cross_val_score(xgb,X_train,Y_train,cv=5)\n",
        "print(cv)\n",
        "print(cv.mean())"
      ],
      "metadata": {
        "id": "W2gLb9PGdRVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression(max_iter = 2000)\n",
        "cv = cross_val_score(lr,X_train,Y_train,cv=5)\n",
        "print(cv)\n",
        "print(cv.mean())"
      ],
      "metadata": {
        "id": "9myxiH-nedJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt = tree.DecisionTreeClassifier(random_state = 1)\n",
        "cv = cross_val_score(dt,X_train,Y_train,cv=5)\n",
        "print(cv)\n",
        "print(cv.mean())"
      ],
      "metadata": {
        "id": "dZ6jM69zff2L",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}